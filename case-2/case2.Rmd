---
title: "Case 2"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

# Summary

## Part 1 

- Through stepwise regression, only `Company size` is a significant feature with a p-value of 0.000919 and a beta of 1.4651.
- This stepwise model has a good fit, as it has an adjusted $R^2$ of 0.7299 and a p-value of 0.0001541.
- Thsis stepwise model generates the following function

$$
\text{Size of Purchase (\$1,000s)} = 128.7 + 1.4651 \times \text{Company Size (\$millions sales)} -41.07 \times \text{Similar Products}
$$

## Part 2

- There seems to be no linear relationship between the dependent variable (`Average sales`) and independent variable (`Hours worked per week` and `Number of Customers`)
- Plotting `Average sales` against `Hours worked per week` is heteroscedatic, therefore ruling out the use of OLS
- However oddly, using OLS, we could get a (almost) good fit of p-value of 0.002616 and Adjusted R-squared of 0.9786 with the following equation (with *S* as the `Average sales`, *C* as the `Number of Customers`, *H* as the `Hours worked per week`). But this equation is kind of useless as the RESET test would show these parameters to be invalid.

$$
\begin{split}
S = &4.804(10^3) \\
+ &2.586(10^{-1}) C - 4.999(10^{-6}) C^3 \\
- &3.863(10^2)H + 1.028(10^1) H^2 - 9.065(10^{-2}) H^3 \\
+ &6.025(10^{-19}) e^{H}
\end{split}
$$

## Part 3

- There is a quadratic relationship between `Sales ($ million)` and `Number of Employees`. 
- The quadratic model would follow the following model

$$
\text{Average Sales} = -93.21 + 1.4554 \times \text{No. of employees} + -0.0040 \times \text{No. of employees}^2
$$

- This quadratic model would be have an Adjusted R-squared of 0.7535 and p-value: 0.003084

\newpage

# Works

Importing various libraries
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(leaps)
library(MASS)
```

## Part 1

Import the data set and then generate `stepwise regression` model

```{r}
data_part1 <- read.csv('csv/part1.csv', header = TRUE)
# Fit the full model 
full.model_part1 <- lm(Size.of.Purchase...1.000s. ~ ., data=data_part1)
# Stepwise regression model
step.model_part1 <- stepAIC(full.model_part1, direction = "both", trace = FALSE)
summary(step.model_part1)
```

\newpage

## Part 2

Get the data and divide up the columns of data

```{r}
data_part2 <- read.csv('csv/part2.csv', header = TRUE)
# split the data into various columns
avg_sales <- data_part2$Average.Sales....million.
hours_work <- data_part2$Hours.Worked.per.Week
no_customer <- data_part2$Number.of.Customers
```

Plot the `Average Sales ($ million)` against `Hours Worked per Week`

```{r}
plot(hours_work, avg_sales,
     main = "Average sales and hours worked per week",
     ylab = "Average Sales",
     xlab = "Hours worked",
     pch=19)
sales_hour_model <- lm(data_part2$Average.Sales....million. ~ 
                         data_part2$Hours.Worked.per.Week)
```

We see that the data maybe **heteroscedatic** and we would use the [Breuschâ€“Pagan test](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test) and [White Test](https://en.wikipedia.org/wiki/White_test) to test whether they are actually heteroscedatic.

```{r message=FALSE, warning=FALSE}
library(lmtest)
library(skedastic)
print(bptest(sales_hour_model, 
      data_part2$Average.Sales....million.~data_part2$Hours.Worked.per.Week,
      data=data_part2))
print(white_lm(sales_hour_model)) 
```

From the two test, we see that the data is **heteroscedatic**. Therefore, any method of regression using OLS directly on this set of data would be unfavoured. Then we plot the `Average Sales ($ million)` against `Number of Customers`

```{r}
plot(log(no_customer), avg_sales, 
     main = "Average sales and Number of Customers",
     ylab = "Average Sales",
     xlab = "Number of Customers",
     pch=19)
```

We see that the data has a possible outlier at (188, 13.8). There seems to be a weak relationship between the `Average Sales ($ million)` and `Number of Customers`. Then, we run a stepwise regression (however unwillingly, as it is unfavourable to run an OLS model with data that is heteroscedatic)

```{r}
full.model_part2 <- lm(avg_sales ~ poly(no_customer,3,raw=TRUE) + poly(hours_work,3,raw=TRUE) 
                       + log(hours_work))
step.model_part2 <- stepAIC(full.model_part2, direction = "both", trace = FALSE)
summary(step.model_part2)
```

From this, we could see there are various insignificant variables, but oddly, it seems that the `poly()` function has caused the `stepAIC()` function unable to take away values such as $\text{Hours of work} ^ 2$ in the process, therefore we tried to run it with various arrays generated individually. 

```{r}
no_customer_2 <- no_customer ^ 2
no_customer_3 <- no_customer ^ 3
hours_work_2 <- hours_work ^ 2
hours_work_3 <- hours_work ^ 3
log_hours <- exp(hours_work)
full.model_part2 <- lm(avg_sales ~ 
                         no_customer + no_customer_2 + no_customer_3 +
                         hours_work + hours_work_2 + hours_work_3 +
                         log_hours)
step.model_part2 <- stepAIC(full.model_part2, direction = "both", trace = FALSE)
summary(step.model_part2)
```

It worked! At least at the face of it, with an Adjusted R-squared of 0.9786 and p-value of 0.002616. Also, the model has none insignificant values, but still we would run a [Ramsey RESET test](https://en.wikipedia.org/wiki/Ramsey_RESET_test) and [VIF](https://en.wikipedia.org/wiki/Variance_inflation_factor)

```{r message=FALSE, warning=FALSE}
library(car)
print(vif(step.model_part2))
```

All the variables with relation to hours of work, has a significantly higher value than the rest of the variables.

```{r}
print(resettest(step.model_part2))
```

Our worst fears. The RESET test p-value would indicate that the model has misspecified in the sense that the data generating process might be better approximated by a polynomial or another non-linear functional form. 

To this point, I recall that a heteroskedatic data samples and the collinearity of the features would actually distort the p-value of the OLS model. Therefore, now we would use a [Lasso Regression](https://en.wikipedia.org/wiki/Lasso_(statistics)) to test for the best fit model.

## Part 2 - Lasso Regression



```{r}

```



\newpage

## Part 3

Get the data and plot the scatter plot

```{r}
data_part3 <- read.csv('csv/part3.csv', header = TRUE)

plot(data_part3$Number.of.Employees,
     data_part3$Sales..............million., 
     main = "Sales against Number of employees",
     ylab = "Sales",
     xlab = "Number of employees", pch=19)
```

There seems to be a non-linear relationship between `Sales ($ million)` and `Number of Employees`. Therefore we try to do a non-linear regression for it. This looks like a quadratic relationship.

```{r}
sales <- data_part3$Sales..............million.
no_employees <- data_part3$Number.of.Employees


full.model_part3 <- lm(sales ~ poly(no_employees,2,raw=TRUE))
summary(full.model_part3)
```

We would also draw out the final graph for it.

```{r}
#generate range of 50 numbers starting from 30 and ending at 160
xx <- seq(110,220, length=50)
plot(no_employees, sales, pch=19,
     xlab="Number of Employees",
     ylab="Sales",
     ylim=c(20,40))
lines(xx, predict(full.model_part3, data.frame(no_employees=xx)), col="red")
```


